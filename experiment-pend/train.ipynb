{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamiltonian Neural Networks | 2019\n",
    "# Sam Greydanus, Misko Dzamba, Jason Yosinski\n",
    "\n",
    "import torch, argparse\n",
    "import numpy as np\n",
    "\n",
    "import os, sys\n",
    "THIS_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PARENT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "sys.path.append(PARENT_DIR)\n",
    "\n",
    "from nn_models import MLP\n",
    "from hnn import HNN\n",
    "from data import get_dataset\n",
    "from utils import L2_loss, rk4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument('--input_dim', default=2, type=int, help='dimensionality of input tensor')\n",
    "    parser.add_argument('--hidden_dim', default=200, type=int, help='hidden dimension of mlp')\n",
    "    parser.add_argument('--learn_rate', default=1e-3, type=float, help='learning rate')\n",
    "    parser.add_argument('--nonlinearity', default='tanh', type=str, help='neural net nonlinearity')\n",
    "    parser.add_argument('--total_steps', default=2000, type=int, help='number of gradient steps')\n",
    "    parser.add_argument('--print_every', default=200, type=int, help='number of gradient steps between prints')\n",
    "    parser.add_argument('--name', default='pend', type=str, help='only one option right now')\n",
    "    parser.add_argument('--baseline', dest='baseline', action='store_true', help='run baseline or experiment?')\n",
    "    parser.add_argument('--use_rk4', dest='use_rk4', action='store_true', help='integrate derivative with RK4')\n",
    "    parser.add_argument('--verbose', dest='verbose', action='store_true', help='verbose?')\n",
    "    parser.add_argument('--field_type', default='solenoidal', type=str, help='type of vector field to learn')\n",
    "    parser.add_argument('--seed', default=0, type=int, help='random seed')\n",
    "    parser.add_argument('--save_dir', default=THIS_DIR, type=str, help='where to save the trained model')\n",
    "    parser.set_defaults(feature=True)\n",
    "    try:\n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        return parser.parse_args([])\n",
    "    except:\n",
    "        return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    \n",
    "    # set random seed\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    # init model and optimizer\n",
    "    if args.verbose:\n",
    "        print(\"Training baseline model:\" if args.baseline else \"Training HNN model:\")\n",
    "\n",
    "    output_dim = args.input_dim if args.baseline else 2\n",
    "    nn_model = MLP(args.input_dim, args.hidden_dim, output_dim, args.nonlinearity)\n",
    "    model = HNN(args.input_dim, differentiable_model=nn_model,\n",
    "                field_type=args.field_type, baseline=args.baseline)\n",
    "    optim = torch.optim.Adam(model.parameters(), args.learn_rate, weight_decay=1e-4)\n",
    "\n",
    "    # arrange data\n",
    "    data = get_dataset(seed=args.seed)\n",
    "    x = torch.tensor( data['x'], requires_grad=True, dtype=torch.float32)\n",
    "    test_x = torch.tensor( data['test_x'], requires_grad=True, dtype=torch.float32)\n",
    "    dxdt = torch.Tensor(data['dx'])\n",
    "    test_dxdt = torch.Tensor(data['test_dx'])\n",
    "\n",
    "    # vanilla train loop\n",
    "    stats = {'train_loss': [], 'test_loss': []}\n",
    "    for step in range(args.total_steps+1):\n",
    "        \n",
    "        # train step\n",
    "        dxdt_hat = model.rk4_time_derivative(x) if args.use_rk4 else model.time_derivative(x)\n",
    "        loss = L2_loss(dxdt, dxdt_hat)\n",
    "        loss.backward() ; optim.step() ; optim.zero_grad()\n",
    "        \n",
    "        # run test data\n",
    "        test_dxdt_hat = model.rk4_time_derivative(test_x) if args.use_rk4 else model.time_derivative(test_x)\n",
    "        test_loss = L2_loss(test_dxdt, test_dxdt_hat)\n",
    "\n",
    "        # logging\n",
    "        stats['train_loss'].append(loss.item())\n",
    "        stats['test_loss'].append(test_loss.item())\n",
    "        if args.verbose and step % args.print_every == 0:\n",
    "            print(\"step {}, train_loss {:.4e}, test_loss {:.4e}\".format(step, loss.item(), test_loss.item()))\n",
    "\n",
    "    train_dxdt_hat = model.time_derivative(x)\n",
    "    train_dist = (dxdt - train_dxdt_hat)**2\n",
    "    test_dxdt_hat = model.time_derivative(test_x)\n",
    "    test_dist = (test_dxdt - test_dxdt_hat)**2\n",
    "    print('Final train loss {:.4e} +/- {:.4e}\\nFinal test loss {:.4e} +/- {:.4e}'\n",
    "        .format(train_dist.mean().item(), train_dist.std().item()/np.sqrt(train_dist.shape[0]),\n",
    "                        test_dist.mean().item(), test_dist.std().item()/np.sqrt(test_dist.shape[0])))\n",
    "\n",
    "    # save\n",
    "    os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None\n",
    "    label = '-baseline' if args.baseline else '-hnn'\n",
    "    label = '-rk4' + label if args.use_rk4 else label\n",
    "    path = '{}/{}{}.tar'.format(args.save_dir, args.name, label)\n",
    "    torch.save(model.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "base2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
